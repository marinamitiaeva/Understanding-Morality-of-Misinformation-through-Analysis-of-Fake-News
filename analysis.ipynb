{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas matplotlib wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, fnmatch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from statistics import mean\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "import math\n",
    "import json\n",
    "import ast\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('/workspaces/fake_news_analysis/data/preprocessed/fake_preprocessed.csv')\n",
    "real = pd.read_csv('/workspaces/fake_news_analysis/data/preprocessed/real_preprocessed.csv')\n",
    "\n",
    "df = pd.concat([fake, real])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(text):\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "df['processed_text'] = df['processed_text'].apply(string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = ['care', 'fairness', 'loyalty', 'authority', 'sanctity']\n",
    "polarities = ['virtue', 'vice']\n",
    "\n",
    "# dictionaries\n",
    "dictionary_paths = {\n",
    "    'mft': '/workspaces/fake_news_analysis/dictionaries/processed/mft_dictionary.json',\n",
    "    'mfd2': '/workspaces/fake_news_analysis/dictionaries/processed/mfd2_dictionary.json',\n",
    "    'mfd1': '/workspaces/fake_news_analysis/dictionaries/processed/mfd1_dictionary.json',\n",
    "    'emfd': '/workspaces/fake_news_analysis/dictionaries/processed/emfd_dictionary.json',\n",
    "    'ms': '/workspaces/fake_news_analysis/dictionaries/processed/ms_dictionary.json'\n",
    "}\n",
    "\n",
    "\n",
    "loaded_dictionaries = {}\n",
    "for key, path in dictionary_paths.items():\n",
    "    with open(path, 'r') as file:\n",
    "        loaded_dictionaries[key] = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF normalized\n",
    "\n",
    "def score_tf(tokens, dictionary):\n",
    "    # Initialize scores and term frequencies\n",
    "    scores = {dimension: {polarity: 0 for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "    tf = {token: tokens.count(token) for token in set(tokens)}\n",
    "    tracked = {dimension: {polarity: [] for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "\n",
    "    # Calculate maximum term frequency for normalization\n",
    "    max_tf = max(tf.values()) if tf else 1\n",
    "\n",
    "    # Iterate through each token and update scores based on normalized term frequency\n",
    "    for token in set(tokens):  # Iterate through unique tokens for efficiency\n",
    "        for dimension, polarities in dictionary.items():\n",
    "            for polarity, words in polarities.items():\n",
    "                if token in words:\n",
    "                    # Calculate TF normalization factor\n",
    "                    tf_factor = 0.5 + 0.5 * (tf[token] / max_tf)\n",
    "                    # Update scores with TF normalization\n",
    "                    scores[dimension][polarity] += tf_factor\n",
    "                    if token not in tracked[dimension][polarity]:\n",
    "                        tracked[dimension][polarity].append(token)\n",
    "\n",
    "    vector = []\n",
    "    for dimension in scores.keys():\n",
    "        for polarity in scores[dimension].keys():\n",
    "            vector.append(scores[dimension][polarity])\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['mft', 'mfd2', 'mfd1']\n",
    "\n",
    "for key in keys:\n",
    "    if key in loaded_dictionaries:\n",
    "      df[key] = df['processed_text'].apply(lambda x: score_tf(x, loaded_dictionaries[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prob(tokens, dictionary):\n",
    "    prob_sums = {dimension: {polarity: 0 for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "    word_counts = {dimension: {polarity: 0 for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "    tracked = {dimension: {polarity: [] for polarity in dictionary[dimension]} for dimension in dictionary}\n",
    "\n",
    "    # Process tokens against the dictionary\n",
    "    for token in tokens:\n",
    "        for dimension, polarities in dictionary.items():\n",
    "            for polarity, words_probs in polarities.items():\n",
    "                if isinstance(words_probs, list) and words_probs:\n",
    "                    for word_prob in words_probs:\n",
    "                        if token == word_prob[0]:  # If the token matches the word\n",
    "                            prob_sums[dimension][polarity] += word_prob[1]\n",
    "                            word_counts[dimension][polarity] += 1\n",
    "                            if token not in tracked[dimension][polarity]:\n",
    "                                tracked[dimension][polarity].append(token)\n",
    "\n",
    "    # Calculate averages based on tracked order\n",
    "    avg_probs = {}\n",
    "    for dimension in dictionary:  # Ensure dimension order matches 'tracked'\n",
    "        avg_probs[dimension] = {}\n",
    "        for polarity in dictionary[dimension]:  # Ensure polarity order matches 'tracked'\n",
    "            if word_counts[dimension][polarity] > 0:\n",
    "                avg = prob_sums[dimension][polarity] / word_counts[dimension][polarity]\n",
    "            else:\n",
    "                avg = 0\n",
    "            avg_probs[dimension][polarity] = avg\n",
    "\n",
    "    # Create the vector based on the same order as 'tracked'\n",
    "    vector = []\n",
    "    for dimension, polarities in tracked.items():  # Follow the order in 'tracked'\n",
    "        for polarity in polarities:  # Follow the order in 'tracked'\n",
    "            vector.append(avg_probs[dimension][polarity])\n",
    "\n",
    "    # Ensure the vector is 10-dimensional\n",
    "    vector = vector[:10] if len(vector) > 10 else vector + [0] * (10 - len(vector))\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emfd'] = df['processed_text'].apply(lambda x: score_prob(x, loaded_dictionaries['emfd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_range(tokens, dictionary):\n",
    "\n",
    "    # Initialize scores and counts\n",
    "    scores = {f'{dimension}_{polarity}': [] for dimension in dimensions for polarity in polarities}\n",
    "\n",
    "    # Iterate through tokens and update scores\n",
    "    for token in tokens:\n",
    "        for dimension in dimensions:\n",
    "            for polarity in polarities:\n",
    "                key = f'{dimension}_{polarity}'\n",
    "                if dimension in dictionary and polarity in dictionary[dimension]:\n",
    "                    for word, score in dictionary[dimension][polarity]:\n",
    "                        if token == word:\n",
    "                            scores[key].append(score)\n",
    "\n",
    "    # Compute averages for each dimension and polarity\n",
    "    averages = []\n",
    "    for key in scores:\n",
    "        if scores[key]:\n",
    "            avg_score = sum(scores[key]) / len(scores[key])\n",
    "        else:\n",
    "            avg_score = 0  # Default to 0 if no scores were found\n",
    "        averages.append(avg_score)\n",
    "\n",
    "    # Ensure the result is 10-dimensional\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ms'] = df['processed_text'].apply(lambda x: score_range(x, loaded_dictionaries['ms']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/workspaces/fake_news_analysis/data/preprocessed/data_vectorized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flattened_morality_words = {}\n",
    "for category, subcats in enh_mfd1_dictionary.items():\n",
    "    for subcat, words in subcats.items():\n",
    "        for word in words:\n",
    "            flattened_morality_words[word] = category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
