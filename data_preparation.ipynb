{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk_resources = ['stopwords', 'punkt', 'averaged_perceptron_tagger', 'wordnet']\n",
    "for resource in nltk_resources:\n",
    "    nltk.download(resource, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_data_path = '/workspaces/fake_news_analysis/data/'\n",
    "raw_data_path = f'{base_data_path}raw/'\n",
    "preprocessed_data_path = f'{base_data_path}preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read news data\n",
    "def load_news_data(type, category):\n",
    "    path = f'{raw_data_path}{type}_{category}_news.csv'\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "news_datasets = {f\"{type}_{category}\": load_news_data(type, category)\n",
    "                 for type in ['politifact', 'gossipcop']\n",
    "                 for category in ['real', 'fake']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politifact_real rows: 624, nulls: 182\n",
      "politifact_fake rows: 432, nulls: 89\n",
      "gossipcop_real rows: 16817, nulls: 3274\n",
      "gossipcop_fake rows: 5323, nulls: 1070\n"
     ]
    }
   ],
   "source": [
    "# Print dataset shapes\n",
    "for name, dataset in news_datasets.items():\n",
    "    print(f\"{name} rows: {dataset.shape[0]},\", f\"nulls: {dataset['text'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words_dic = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words_dic]\n",
    "\n",
    "    # Part-of-Speech tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Normalize case (except for proper nouns)\n",
    "    tokens = [word for  word, tag in tagged_tokens if tag != 'NNP' and tag != 'NNPS']\n",
    "\n",
    "    # Remove punctuation and numbers, keeping only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Lemmatize tokens using the appropriate WordNet tag\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def preprocess_data(news_datasets, label_mappings):\n",
    "    processed_datasets = {}\n",
    "    for key, df in news_datasets.items():\n",
    "        df_clean = df[['id', 'text']].dropna()  # Ensure text column is not empty\n",
    "        df_clean['processed_text'] = df_clean['text'].apply(preprocess_text)\n",
    "        \n",
    "        # Apply all label mappings\n",
    "        for mapping in label_mappings:\n",
    "            for label_key, label_value in mapping.items():\n",
    "                if label_key in key:\n",
    "                    df_clean[label_value['label']] = label_value['value']\n",
    "        \n",
    "        processed_datasets[key] = df_clean\n",
    "    \n",
    "    return processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappings = [\n",
    "    {'politifact': {'label': 'topic', 'value': 'politics'}, 'gossipcop': {'label': 'topic', 'value': 'entertainment'}},\n",
    "    {'real': {'label': 'type', 'value': 'real'}, 'fake': {'label': 'type', 'value': 'fake'}}\n",
    "]\n",
    "\n",
    "# Applying preprocessing and labeling\n",
    "processed_datasets = preprocess_data(news_datasets, label_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate real and fake news data\n",
    "fake = pd.concat([processed_datasets['politifact_fake'], processed_datasets['gossipcop_fake']])\n",
    "real = pd.concat([processed_datasets['politifact_real'], processed_datasets['gossipcop_real']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                               text  \\\n",
      "1  politifact15156  The West Texas Federal Appeals Court, operatin...   \n",
      "3  politifact14355  The former Paralympic athlete reportedly tried...   \n",
      "4  politifact15371  President Trump and his administration just vo...   \n",
      "5  politifact14404  aderito Report\\n\\nReport Change comment\\n\\nRem...   \n",
      "6  politifact13919  About Trendolizer™\\n\\nTrendolizer™ (patent pen...   \n",
      "\n",
      "                                      processed_text     topic  type  \n",
      "1  [west, texas, federal, appeal, court, operatin...  politics  fake  \n",
      "3  [former, paralympic, athlete, reportedly, trie...  politics  fake  \n",
      "4  [president, trump, administration, voted, unit...  politics  fake  \n",
      "5  [aderito, report, report, change, comment, rem...  politics  fake  \n",
      "6  [patent, pending, automatically, scan, interne...  politics  fake  \n",
      "                id                                               text  \\\n",
      "0  politifact14984  SMALL BUSINESS ECONOMIC TRENDS\\n\\nSmall Busine...   \n",
      "1  politifact12944  Username\\n\\nPassword\\n\\nNeed help? Contact the...   \n",
      "2    politifact333  Romney makes pitch, hoping to close deal\\n\\nPh...   \n",
      "3   politifact4358  Democratic Leaders Say House Democrats Are Uni...   \n",
      "4    politifact779  THE NATION’S FISCAL OUTLOOK\\n\\nThe President’s...   \n",
      "\n",
      "                                      processed_text     topic  type  \n",
      "0  [small, business, economic, trend, small, busi...  politics  real  \n",
      "1  [username, password, need, help, contact, cq, ...  politics  real  \n",
      "2  [romney, make, pitch, hoping, close, deal, pho...  politics  real  \n",
      "3  [democratic, leader, say, house, democrat, uni...  politics  real  \n",
      "4  [nation, fiscal, outlook, president, budget, a...  politics  real  \n"
     ]
    }
   ],
   "source": [
    "# Preview data\n",
    "print(fake.head())\n",
    "print(real.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "fake.to_csv(f'{preprocessed_data_path}fake_preprocessed.csv')\n",
    "real.to_csv(f'{preprocessed_data_path}real_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
